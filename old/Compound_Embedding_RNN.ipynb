{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main file to run the experiment 2:\n",
    "Compare MRR and HIT \n",
    "'''\n",
    "\n",
    "#Import Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import Word2Vec\n",
    "import functools\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# Import modules\n",
    "import utils\n",
    "import evaluation\n",
    "# Import Baselines\n",
    "from SimpleRNN import Simple_RNN_baseline\n",
    "from Average_baseline import AVG_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cont declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files Paths\n",
    "type_of_Word2Vec_model = 'CBOW'\n",
    "vector_file_name = 'wiki-db_more50_200'\n",
    "vector_file_name_path = './../model/' + type_of_Word2Vec_model + '/' + vector_file_name\n",
    "train_file_name = 'uni_pair_combine_less100'\n",
    "train_file_path = './../dataset/train_data/'\n",
    "\n",
    "save_model_path = './../model/'\n",
    "x_file = save_model_path + 'Evaluation/' + type_of_Word2Vec_model + '_X_feature.npy'\n",
    "y_file = save_model_path + 'Evaluation/' + type_of_Word2Vec_model + '_Y_label.npy'\n",
    "\n",
    "# Integer Constant\n",
    "MAX_SEQUENCE_LENGTH = 21\n",
    "num_of_epochs = 10\n",
    "batch_size = 1024 \n",
    "validation_split = 0.1\n",
    "# Hyperparameters Setup\n",
    "embedding_dim = 200\n",
    "num_hidden = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(wordvec, main_baseline, x_train_cv, y_train_cv , x_test_cv, y_label_cv):\n",
    "    '''\n",
    "    Function to train main_baseline evaluation in Cross-validation scenario for Experiment 2\n",
    "    Input: \n",
    "            main_baseline: the main baseline that need to be compare with comparison_baseline\n",
    "            x_train_cv: feature matrix (X) for training, shape(90% number_of_data, MAX_SEQUENCE_LENGTH) of word_idx\n",
    "            y_train_cv: label matrix (Y) for training, shape(90% number_of_data, embedding_dim) word vector of compount word\n",
    "            x_test_cv: x_train_cv: feature matrix (X) for testing, shape(10% number_of_data, MAX_SEQUENCE_LENGTH) of word_idx\n",
    "            y_test_cv: label matrix (Y) for testing, shape(10% number_of_data, embedding_dim) word vector of compount word\n",
    "\n",
    "    Output:\n",
    "            MRR: Mean reciprocal rank of the main_baseline\n",
    "            HIT_1: HIT@1 of the main_baseline\n",
    "            HIT_10: HIT@10 of the main_baseline\n",
    "    '''\n",
    "    ## Training Phase\n",
    "    # Train the main_baseline\n",
    "    main_baseline.train(x_train_cv,y_train_cv)\n",
    "\n",
    "    ## Inference Phase\n",
    "    # Predict result of the main_baseline\n",
    "    main_baseline_y_predict = main_baseline.predict(x_test_cv)\n",
    "\n",
    "    \n",
    "    ## Testing \n",
    "    MRR, HIT_1, HIT_10 = evaluation.calculateMRR_HIT(wordvec,y_label_cv,main_baseline_y_predict)\n",
    "    \n",
    "    \n",
    "    return MRR , HIT_1, HIT_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "    # Load the Pretrained Word Vector from Gensim\n",
    "    wordvec = Word2Vec.load(vector_file_name_path) # Load the model from the vector_file_name\n",
    "    wordvec.wv.init_sims(replace=True)\n",
    "    print('Loaded Word2Vec model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  968009\n"
     ]
    }
   ],
   "source": [
    "    # Get Vocabulary Size\n",
    "    vocab_size = len(wordvec.wv.vocab)\n",
    "    print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../model/Evaluation/CBOW_X_feature.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-09bf31da8bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_label_data_from_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordvec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Preprocess the input data for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_from_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_file\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# Load input data from numpy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/NII/Word_Embedding/Compositional_Word_Embedding/old/utils.py\u001b[0m in \u001b[0;36mload_data_from_numpy\u001b[0;34m(feature_file, label_file)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mlabel_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompound\u001b[0m \u001b[0mword\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     '''\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Env/python3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './../model/Evaluation/CBOW_X_feature.npy'"
     ]
    }
   ],
   "source": [
    "    # Prepare Train_data\n",
    "    fname = os.path.join(train_file_path,train_file_name)\n",
    "    label = utils.load_label_data_from_text_file(fname,wordvec,MAX_SEQUENCE_LENGTH) # Preprocess the input data for the model\n",
    "    X, Y = utils.load_data_from_numpy(x_file, y_file)            # Load input data from numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Word2Vec Gensim Model to Embedding Matrix to input into RNN\n",
    "    embedding_matrix = utils.Word2VecTOEmbeddingMatrix(wordvec,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e8aa92117439>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Define train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         x_train_cv = X[train_idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Do Cross Validation\n",
    "    kFold = KFold(n_splits = 10)\n",
    "    #Init the Accuracy dictionary = {}\n",
    "    accuracy = {}\n",
    "    accuracy['MRR'] = np.zeros(10)\n",
    "    accuracy['HIT_1'] = np.zeros(10)\n",
    "    accuracy['HIT_10'] = np.zeros(10)\n",
    "    idx = 0 # Index of accuracy\n",
    "    for train_idx, test_idx in kFold.split(X,Y):\n",
    "        # Define train and test data\n",
    "        print(train_idx)\n",
    "        print(test_idx)\n",
    "        \n",
    "#         x_train_cv = X[train_idx]\n",
    "#         x_test_cv  = X[test_idx]\n",
    "        \n",
    "#         y_train_cv = Y[train_idx]\n",
    "#         y_test_cv  = Y[test_idx]\n",
    "#         y_label_cv = [label[j] for j in test_idx]\n",
    "\n",
    "#         # Compare two baseline \n",
    "#         # Define two baseline\n",
    "#         main_baseline = Simple_RNN_baseline(type_of_Word2Vec_model,vocab_size,embedding_dim,embedding_matrix,MAX_SEQUENCE_LENGTH) # Init main baseline: SimpleRNN\n",
    "        \n",
    "#         accuracy['MRR'][idx],accuracy['HIT_1'][idx],accuracy['HIT_10'][idx] = train_evaluate(main_baseline, x_train_cv, y_train_cv , x_test_cv,y_label_cv)\n",
    "#         idx += 1\n",
    "#         print('========= Fold {} ============='.format(idx))\n",
    "#         print('MRR: {}'.format(accuracy['MRR'][idx]))\n",
    "#         print('HIT@1: {}'.format(accuracy['HIT_1'][idx]))\n",
    "#         print('HIT@10: {}'.format(accuracy['HIT_10'][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
