{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Main file to run the experiment 2:\n",
    "Compare MRR and HIT \n",
    "'''\n",
    "\n",
    "#Import Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import Word2Vec\n",
    "import functools\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# Import modules\n",
    "import utils\n",
    "import evaluation\n",
    "# Import Baselines\n",
    "from SimpleRNN import Simple_RNN_baseline\n",
    "from Average_baseline import AVG_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cont declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files Paths\n",
    "type_of_Word2Vec_model = 'CBOW'\n",
    "vector_file_name = 'wiki-db_more50_200'\n",
    "vector_file_name_path = './../model/' + type_of_Word2Vec_model + '/' + vector_file_name\n",
    "train_file_name = 'uni_pair_combine_less100'\n",
    "train_file_path = './../dataset/train_data/'\n",
    "\n",
    "save_model_path = './../model/'\n",
    "x_file = save_model_path + 'Evaluation/' + type_of_Word2Vec_model + '_X_feature.npy'\n",
    "y_file = save_model_path + 'Evaluation/' + type_of_Word2Vec_model + '_Y_label.npy'\n",
    "\n",
    "# Integer Constant\n",
    "MAX_SEQUENCE_LENGTH = 21\n",
    "num_of_epochs = 5\n",
    "batch_size = 1024 \n",
    "validation_split = 0.01\n",
    "# Hyperparameters Setup\n",
    "embedding_dim = 200\n",
    "num_hidden = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(wordvec, main_baseline, x_train_cv, y_train_cv , x_test_cv, y_label_cv):\n",
    "    '''\n",
    "    Function to train main_baseline evaluation in Cross-validation scenario for Experiment 2\n",
    "    Input: \n",
    "            main_baseline: the main baseline that need to be compare with comparison_baseline\n",
    "            x_train_cv: feature matrix (X) for training, shape(90% number_of_data, MAX_SEQUENCE_LENGTH) of word_idx\n",
    "            y_train_cv: label matrix (Y) for training, shape(90% number_of_data, embedding_dim) word vector of compount word\n",
    "            x_test_cv: x_train_cv: feature matrix (X) for testing, shape(10% number_of_data, MAX_SEQUENCE_LENGTH) of word_idx\n",
    "            y_test_cv: label matrix (Y) for testing, shape(10% number_of_data, embedding_dim) word vector of compount word\n",
    "\n",
    "    Output:\n",
    "            MRR: Mean reciprocal rank of the main_baseline\n",
    "            HIT_1: HIT@1 of the main_baseline\n",
    "            HIT_10: HIT@10 of the main_baseline\n",
    "    '''\n",
    "    ## Training Phase\n",
    "    # Train the main_baseline\n",
    "    main_baseline.train(x_train_cv,y_train_cv,num_of_epochs,batch_size,validation_split)\n",
    "\n",
    "    ## Inference Phase\n",
    "    # Predict result of the main_baseline\n",
    "    main_baseline_y_predict = main_baseline.predict(x_test_cv)\n",
    "\n",
    "    \n",
    "    ## Testing \n",
    "    MRR, HIT_1, HIT_10 = evaluation.calculateMRR_HIT(wordvec,y_label_cv,main_baseline_y_predict)\n",
    "    \n",
    "    \n",
    "    return MRR , HIT_1, HIT_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "# Load the Pretrained Word Vector from Gensim\n",
    "wordvec = Word2Vec.load(vector_file_name_path) # Load the model from the vector_file_name\n",
    "wordvec.wv.init_sims(replace=True)\n",
    "print('Loaded Word2Vec model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  968009\n"
     ]
    }
   ],
   "source": [
    "# Get Vocabulary Size\n",
    "vocab_size = len(wordvec.wv.vocab)\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Prepare Train_data\n",
    "fname = os.path.join(train_file_path,train_file_name)\n",
    "label = utils.load_label_data_from_text_file(fname,wordvec,MAX_SEQUENCE_LENGTH) # Preprocess the input data for the model\n",
    "X,Y = utils.load_data_from_text_file(fname,wordvec,MAX_SEQUENCE_LENGTH)\n",
    "# X, Y = utils.load_data_from_numpy(x_file, y_file)            # Load input data from numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Word2Vec Gensim Model to Embedding Matrix to input into RNN\n",
    "embedding_matrix = utils.Word2VecTOEmbeddingMatrix(wordvec,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201970"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201970"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201970"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 21, 200)           193601800 \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 200)               80200     \n",
      "=================================================================\n",
      "Total params: 193,682,000\n",
      "Trainable params: 80,200\n",
      "Non-trainable params: 193,601,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 179955 samples, validate on 1818 samples\n",
      "Epoch 1/5\n",
      "179955/179955 [==============================] - 8s 44us/step - loss: 0.0052 - acc: 0.0296 - val_loss: 0.0049 - val_acc: 0.0468\n",
      "Epoch 2/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0048 - acc: 0.0573 - val_loss: 0.0047 - val_acc: 0.0556\n",
      "Epoch 3/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0047 - acc: 0.0685 - val_loss: 0.0047 - val_acc: 0.0600\n",
      "Epoch 4/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0047 - acc: 0.0746 - val_loss: 0.0046 - val_acc: 0.0721\n",
      "Epoch 5/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0046 - acc: 0.0782 - val_loss: 0.0046 - val_acc: 0.0803\n",
      "Training Done!\n",
      "========= Fold 1 =============\n",
      "MRR: 0.0\n",
      "HIT@1: 0.0\n",
      "HIT@10: 0.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 200)           193601800 \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 200)               80200     \n",
      "=================================================================\n",
      "Total params: 193,682,000\n",
      "Trainable params: 80,200\n",
      "Non-trainable params: 193,601,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 179955 samples, validate on 1818 samples\n",
      "Epoch 1/5\n",
      "179955/179955 [==============================] - 7s 37us/step - loss: 0.0052 - acc: 0.0332 - val_loss: 0.0049 - val_acc: 0.0539\n",
      "Epoch 2/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0048 - acc: 0.0579 - val_loss: 0.0047 - val_acc: 0.0567\n",
      "Epoch 3/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0047 - acc: 0.0660 - val_loss: 0.0047 - val_acc: 0.0759\n",
      "Epoch 4/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0046 - acc: 0.0737 - val_loss: 0.0046 - val_acc: 0.0710\n",
      "Epoch 5/5\n",
      "179955/179955 [==============================] - 3s 19us/step - loss: 0.0046 - acc: 0.0790 - val_loss: 0.0046 - val_acc: 0.0803\n",
      "Training Done!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3808dd04769b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmain_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimple_RNN_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_of_Word2Vec_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Init main baseline: SimpleRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MRR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HIT_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HIT_10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cv\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_test_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_label_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'========= Fold {} ============='\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d9e374821d89>\u001b[0m in \u001b[0;36mtrain_evaluate\u001b[0;34m(wordvec, main_baseline, x_train_cv, y_train_cv, x_test_cv, y_label_cv)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m## Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mMRR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIT_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIT_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculateMRR_HIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_label_cv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmain_baseline_y_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/an_workspace/Compositional_Word_Embedding/old/evaluation.py\u001b[0m in \u001b[0;36mcalculateMRR_HIT\u001b[0;34m(wordvec, label, baseline_predict)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompound_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRanking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompound_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mMRR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMRR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/an_workspace/Compositional_Word_Embedding/old/evaluation.py\u001b[0m in \u001b[0;36mgetRanking\u001b[0;34m(wordvec, compound_word, vec)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     '''\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtop10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/an_workspace/Env/python3-gpu/lib/python3.4/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     @deprecated(\n",
      "\u001b[0;32m~/an_workspace/Env/python3-gpu/lib/python3.4/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrestrict_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Do Cross Validation\n",
    "kFold = KFold(n_splits = 10)\n",
    "#Init the Accuracy dictionary = {}\n",
    "accuracy = {}\n",
    "accuracy['MRR'] = np.zeros(10)\n",
    "accuracy['HIT_1'] = np.zeros(10)\n",
    "accuracy['HIT_10'] = np.zeros(10)\n",
    "idx = 0 # Index of accuracy\n",
    "for train_idx, test_idx in kFold.split(X,Y):\n",
    "    # Define train and test data\n",
    "#     print(train_idx)\n",
    "#     print(test_idx)\n",
    "\n",
    "    x_train_cv = X[train_idx]\n",
    "    x_test_cv  = X[test_idx]\n",
    "\n",
    "    y_train_cv = Y[train_idx]\n",
    "    y_test_cv  = Y[test_idx]\n",
    "    y_label_cv = [label[j] for j in test_idx]\n",
    "\n",
    "    # Compare two baseline \n",
    "    # Define two baseline\n",
    "    main_baseline = Simple_RNN_baseline(type_of_Word2Vec_model,vocab_size,embedding_dim,embedding_matrix,MAX_SEQUENCE_LENGTH) # Init main baseline: SimpleRNN\n",
    "\n",
    "    accuracy['MRR'][idx],accuracy['HIT_1'][idx],accuracy['HIT_10'][idx] = train_evaluate(wordvec, main_baseline, x_train_cv, y_train_cv , x_test_cv,y_label_cv)\n",
    "    idx += 1\n",
    "    print('========= Fold {} ============='.format(idx))\n",
    "    print('MRR: {}'.format(accuracy['MRR'][idx]))\n",
    "    print('HIT@1: {}'.format(accuracy['HIT_1'][idx]))\n",
    "    print('HIT@10: {}'.format(accuracy['HIT_10'][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-gpu",
   "language": "python",
   "name": "python3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
