{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import Word2Vec\n",
    "import functools\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cont declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_Word2Vec_model = 'SG'\n",
    "vector_file_name = 'wiki-db_more50_200'\n",
    "vector_file_name_path = './../model/' + type_of_Word2Vec_model + '/' + vector_file_name\n",
    "MAX_SEQUENCE_LENGTH = 21\n",
    "num_of_epochs = 5\n",
    "batch_size = 1024 \n",
    "\n",
    "train_file_name = 'uni_pair_combine'\n",
    "train_file_path = './../dataset/train_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "num_hidden = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_file_name,wordvec):\n",
    "    '''\n",
    "    Create training data for the network.\n",
    "    Input:\n",
    "    Output: x_train , y_train\n",
    "    '''\n",
    "    # initiate the return values\n",
    "    \n",
    "    #Read data\n",
    "    fin = open(input_file_name,'r', encoding = 'utf-8').read().split('\\n')\n",
    "#     print('First sentence: ', fin[0])\n",
    "    num_of_train_sample = len(fin)\n",
    "    \n",
    "    # Initiate the return values\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "\n",
    "    # Load data\n",
    "    count = 0\n",
    "    inVocab = 0\n",
    "    OOV_count = 0\n",
    "    with open(input_file_name,'r', encoding = 'utf-8') as fin:\n",
    "        for line in fin:\n",
    "            tmp = line.split('\\t')\n",
    "            y_string = tmp[0]\n",
    "            x_string = tmp[1].lower().strip('\\n').split(' ')\n",
    "            if len(x_string) < 2:\n",
    "                continue\n",
    "            if y_string in wordvec.wv:\n",
    "                y_train.append(wordvec.wv[y_string])\n",
    "                inVocab += 1\n",
    "            else:\n",
    "                #y_train.append(wordvec.wv['UNKNOWN'])\n",
    "                #OOV_count += 1\n",
    "                continue\n",
    "            # change Text into Integer\n",
    "            x_train_line = []\n",
    "            \n",
    "            for sample in x_string:\n",
    "                if sample in wordvec.wv:\n",
    "                    x_train_line.append(wordvec.wv.vocab[sample].index)\n",
    "                else:\n",
    "                    x_train_line.append(wordvec.wv.vocab['unknown'].index)\n",
    "            x_train.append(x_train_line)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Padding\n",
    "    x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    y_train = np.array(y_train)\n",
    "#     print('Not in Vocab: ',OOV_count)\n",
    "#     print('in Vocab: ',inVocab)\n",
    "    \n",
    "    # return x_train, y_train\n",
    "    return x_train , y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecTOEmbeddingMatrix(wordvec, embedding_dim):\n",
    "    model = wordvec\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), embedding_dim))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Simple RNN network without attention\n",
    "def init_rnn_model(vocab_size, embedding_dim, embedding_matrix, MAX_SEQUENCE_LENGTH ):\n",
    "    model =  Sequential() # Define Sequential Model\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    model.add(embedding_layer) # Add the Embedding layers to \n",
    "    model.add(SimpleRNN(embedding_dim, return_sequences = False))\n",
    "    print(model.summary())\n",
    "    model.compile(loss='mean_squared_error'\n",
    "              ,optimizer='rmsprop'\n",
    "              ,metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "# Load the Pretrained Word Vector from Gensim\n",
    "wordvec = Word2Vec.load(vector_file_name_path) # Load the model from the vector_file_name\n",
    "wordvec.wv.init_sims(replace=True)\n",
    "print('Loaded Word2Vec model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  968009\n"
     ]
    }
   ],
   "source": [
    "# Get Vocabulary Size\n",
    "vocab_size = len(wordvec.wv.vocab)\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('throne', 0.704645037651062),\n",
       " ('prince', 0.679724931716919),\n",
       " ('ruler', 0.6674383282661438),\n",
       " ('kings', 0.6557703018188477),\n",
       " ('DBPEDIA_ID/Hattusili_III', 0.6556156873703003),\n",
       " ('DBPEDIA_ID/Mursili_II', 0.6515605449676514),\n",
       " ('DBPEDIA_ID/Swasawke', 0.6443279981613159),\n",
       " ('DBPEDIA_ID/Vikramaditya', 0.6402791142463684),\n",
       " ('reign', 0.6358672380447388),\n",
       " ('DBPEDIA_ID/Arshak_II', 0.635525107383728)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.wv.similar_by_word('king',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRanking(wordvec, compound_word, vec):\n",
    "    '''\n",
    "    Calculate the rank of the vector using the similar_by_vector function from Gensim\n",
    "    Input:\n",
    "        wordvec: Gensim word2vec model\n",
    "        compound_word: the compound word need to be compare\n",
    "        vec: the estimated vector of that compound word\n",
    "    Output:\n",
    "        rank: the rank of that compound_word when calculate the similarity by word\n",
    "        if rank > 10: return 11\n",
    "    similar_by_vector(vector, topn=10, restrict_vocab=None)\n",
    "    Find the top-N most similar words by vector.\n",
    "\n",
    "    Parameters:\t\n",
    "    vector (numpy.array) – Vector from which similarities are to be computed.\n",
    "    topn ({int, False}, optional) – Number of top-N similar words to return. If topn is False, similar_by_vector returns the vector of similarity scores.\n",
    "    restrict_vocab (int, optional) – Optional integer which limits the range of vectors which are searched for most-similar values. For example, restrict_vocab=10000 would only check the first 10000 word vectors in the vocabulary order. (This may be meaningful if you’ve sorted the vocabulary by descending frequency.)\n",
    "    Returns:\t\n",
    "    Sequence of (word, similarity).\n",
    "\n",
    "    Return type:\t\n",
    "    list of (str, float)\n",
    "    '''\n",
    "    top10 = wordvec.wv.similar_by_vector(vec,topn=10)\n",
    "    for i, word_tuple in enumerate(top10):\n",
    "        word = word_tuple[0]\n",
    "        if word == compound_word:\n",
    "            return i+1\n",
    "\n",
    "    return 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = wordvec.wv['king']\n",
    "getRanking(wordvec,'throne',vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
