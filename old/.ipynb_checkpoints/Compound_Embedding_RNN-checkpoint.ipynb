{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import Word2Vec\n",
    "import functools\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cont declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_Word2Vec_model = 'CBOW'\n",
    "vector_file_name = 'wiki-db_more50_200'\n",
    "vector_file_name_path = './../model/' + type_of_Word2Vec_model + '/' + vector_file_name\n",
    "MAX_SEQUENCE_LENGTH = 21\n",
    "\n",
    "train_file_name = 'uni_pair_combine_less100'\n",
    "train_file_path = './../dataset/train_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "num_hidden = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_file_name,wordvec):\n",
    "    '''\n",
    "    Create training data for the network.\n",
    "    Input:\n",
    "    Output: x_train , y_train\n",
    "    '''\n",
    "    # initiate the return values\n",
    "    \n",
    "    #Read data\n",
    "    fin = open(input_file_name,'r', encoding = 'utf-8').read().split('\\n')\n",
    "#     print('First sentence: ', fin[0])\n",
    "    num_of_train_sample = len(fin)\n",
    "    \n",
    "    # Initiate the return values\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "\n",
    "    # Load data\n",
    "    count = 0\n",
    "    with open(input_file_name,'r', encoding = 'utf-8') as fin:\n",
    "        for line in fin:\n",
    "            tmp = line.split('\\t')\n",
    "            y_string = tmp[0]\n",
    "            x_string = tmp[1].lower().strip('\\n').split(' ')\n",
    "#             print('Y: ',y_string)\n",
    "#             print('X: ',x_string)\n",
    "            if y_string.lower() in wordvec.wv:\n",
    "                y_train.append(wordvec.wv[y_string])\n",
    "            else:\n",
    "                y_train.append(wordvec.wv['UNKNOWN'])\n",
    "            # change Text into Integer\n",
    "            x_train_line = []\n",
    "            for sample in x_string:\n",
    "                if sample in wordvec.wv:\n",
    "                    x_train_line.append(wordvec.wv.vocab[sample].index)\n",
    "                else:\n",
    "                    x_train_line.append(wordvec.wv.vocab['unknown'].index)\n",
    "            x_train.append(x_train_line)\n",
    "            count += 1\n",
    "            if count > 100:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    # Padding\n",
    "    x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # return x_train, y_train\n",
    "    return x_train , y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecTOEmbeddingMatrix(wordvec, embedding_dim):\n",
    "    model = wordvec\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), embedding_dim))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Simple RNN network without attention\n",
    "def init_rnn_model(vocab_size, embedding_dim, embedding_matrix, MAX_SEQUENCE_LENGTH ):\n",
    "    model =  Sequential() # Define Sequential Model\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    model.add(embedding_layer) # Add the Embedding layers to \n",
    "    model.add(SimpleRNN())\n",
    "    print(model.summary())\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "# Load the Pretrained Word Vector from Gensim\n",
    "wordvec = Word2Vec.load(vector_file_name_path) # Load the model from the vector_file_name\n",
    "wordvec.wv.init_sims(replace=True)\n",
    "print('Loaded Word2Vec model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  968009\n"
     ]
    }
   ],
   "source": [
    "# Get Vocabulary Size\n",
    "vocab_size = len(wordvec.wv.vocab)\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:  DBPEDIA_ID/Samotua_River\n",
      "X:  ['samotua', 'river']\n",
      "Y:  DBPEDIA_ID/Phyllosticta_golenkinianthes\n",
      "X:  ['phyllosticta', 'golenkinianthes']\n",
      "Y:  DBPEDIA_ID/Chesmensky_Palace\n",
      "X:  ['chesmensky', 'palace']\n",
      "Y:  DBPEDIA_ID/Edith_Mærsk\n",
      "X:  ['edith', 'm', '', 'rsk']\n",
      "Y:  DBPEDIA_ID/Ana_White\n",
      "X:  ['ana', 'white']\n",
      "Y:  DBPEDIA_ID/Joe_Somebody\n",
      "X:  ['joe', 'somebody']\n",
      "Y:  DBPEDIA_ID/Chesterfield_Township,_Macoupin_County,_Illinois\n",
      "X:  ['chesterfield', 'township', '', 'macoupin', 'county', '', 'illinois']\n",
      "Y:  DBPEDIA_ID/The_French_Lieutenant's_Woman_(film)\n",
      "X:  ['the', 'french', 'lieutenant', '', '', 's', 'woman']\n",
      "Y:  DBPEDIA_ID/Type_100\n",
      "X:  ['type', '100']\n",
      "Y:  DBPEDIA_ID/Canal_Street_Portal\n",
      "X:  ['canal', 'street', 'portal']\n",
      "Y:  DBPEDIA_ID/HMAS_Arunta_(I30)\n",
      "X:  ['hmas', 'arunta']\n",
      "Y:  DBPEDIA_ID/Paleontology_in_Montana\n",
      "X:  ['in', 'montana']\n",
      "Y:  DBPEDIA_ID/Vaejovidae_Thorell,_1876\n",
      "X:  ['vaejovidae', 'thorell', '', '1876']\n",
      "Y:  DBPEDIA_ID/Central_Committee_of_the_Engineering_Workers_Union\n",
      "X:  ['central', 'committee', 'of', 'the', 'engineering', 'workers', 'union']\n",
      "Y:  DBPEDIA_ID/Ottawa-Carleton_Regional_Municipality_elections,_1997\n",
      "X:  ['1997', 'regional', 'elections']\n",
      "Y:  DBPEDIA_ID/The_Beverly_Brothers\n",
      "X:  ['mike', 'enos', 'and', 'wayne', 'bloom']\n",
      "Y:  DBPEDIA_ID/Pseudopais_nigrobasalis\n",
      "X:  ['pseudopais', 'nigrobasalis']\n",
      "Y:  DBPEDIA_ID/Sacco_and_Vanzetti_(disambiguation)\n",
      "X:  ['sacco', 'and', 'vanzetti', '', 'disambiguation']\n",
      "Y:  DBPEDIA_ID/Koninklijke_Nederlandsche_Schaatsenrijders_Bond\n",
      "X:  ['dutch', 'national', 'speed', 'skating', 'association']\n",
      "Y:  DBPEDIA_ID/2009_Meath_Senior_Football_Championship\n",
      "X:  ['2009', 'meath', 'senior', 'football', 'championship']\n",
      "Y:  DBPEDIA_ID/List_of_recurring_characters_in_South_Park#Dr._Alphonse_Mephisto_and_Kevin\n",
      "X:  ['dr', '', 'mephisto']\n",
      "Y:  DBPEDIA_ID/United_States_Capitol_dome#Second_(current)_dome\n",
      "X:  ['united', 'states', 'capitol', 'building']\n",
      "Y:  DBPEDIA_ID/27th_Operations_Group\n",
      "X:  ['27th', 'fighter', 'group']\n",
      "Y:  DBPEDIA_ID/Fort_Point_Light_(Texas)\n",
      "X:  ['fort', 'point', 'light', '', 'texas']\n",
      "Y:  DBPEDIA_ID/folklore_museum\n",
      "X:  ['folklore', 'museum']\n",
      "Y:  DBPEDIA_ID/Luma_alpigena\n",
      "X:  ['luma', 'alpigena']\n",
      "Y:  DBPEDIA_ID/Montana_Raceway_Park\n",
      "X:  ['montana', 'raceway', 'park']\n",
      "Y:  DBPEDIA_ID/out_of_India_model\n",
      "X:  ['out', 'of', 'india', 'model']\n",
      "Y:  DBPEDIA_ID/Museum_of_Avilés_Urban_History\n",
      "X:  ['museum', 'of', 'avil', '', 's', 'urban', 'history']\n",
      "Y:  DBPEDIA_ID/Boxing_at_the_1932_Summer_Olympics_-_Men's_flyweight\n",
      "X:  ['olympic', 'flyweight', 'competition']\n",
      "Y:  DBPEDIA_ID/110_metres_hurdles\n",
      "X:  ['110', 'meters', 'hurdles']\n",
      "Y:  DBPEDIA_ID/Registration_acts_(comics)#1990_Super-human_Registration_Act\n",
      "X:  ['super-powers', 'registration', 'act']\n",
      "Y:  DBPEDIA_ID/Sydney_Christian_Life_Centre\n",
      "X:  ['sydney', 'christian', 'life', 'centre']\n",
      "Y:  DBPEDIA_ID/Mesosemia_maela\n",
      "X:  ['mesosemia', 'maela']\n",
      "Y:  DBPEDIA_ID/First_INA\n",
      "X:  ['first', 'ina']\n",
      "Y:  DBPEDIA_ID/CUNY_Conference_on_Language_Processing\n",
      "X:  ['cuny', 'conference', 'on', 'language', 'processing']\n",
      "Y:  DBPEDIA_ID/I_Guocong\n",
      "X:  ['i', 'guocong']\n",
      "Y:  DBPEDIA_ID/Qışlaq_Abbas\n",
      "X:  ['q', '', '', '', 'laq', 'abbas']\n",
      "Y:  DBPEDIA_ID/RAF_Coastal_Command_during_World_War_II#Coastal_Command_Meteorological_operations\n",
      "X:  ['coastal', 'command', 's', 'meteorological', 'operations']\n",
      "Y:  DBPEDIA_ID/Bob_Mercer_(actor)\n",
      "X:  ['bob', 'mercer']\n",
      "Y:  DBPEDIA_ID/Qaidamestheria_huatingensis\n",
      "X:  ['qaidamestheria', 'huatingensis']\n",
      "Y:  DBPEDIA_ID/2015_BB&T_Atlanta_Open\n",
      "X:  ['atlanta', 'open']\n",
      "Y:  DBPEDIA_ID/Albany_BWP_Highlanders\n",
      "X:  ['albany', 'bwp', 'highlanders']\n",
      "Y:  DBPEDIA_ID/Appendicula_babiensis\n",
      "X:  ['appendicula', 'babiensis']\n",
      "Y:  DBPEDIA_ID/Roberto_Muñoz\n",
      "X:  ['roberto', 'mu', '', 'oz']\n",
      "Y:  DBPEDIA_ID/Ten-hour_Republican_Association\n",
      "X:  ['ten-hour', 'republican', 'association']\n",
      "Y:  DBPEDIA_ID/Etheostoma_fusiforme\n",
      "X:  ['etheostoma', 'fusiforme']\n",
      "Y:  DBPEDIA_ID/St._Peter_(Bruchsal)\n",
      "X:  ['st', '', 'peter']\n",
      "Y:  DBPEDIA_ID/Finis_Mitchell\n",
      "X:  ['finis', 'mitchell']\n",
      "Y:  DBPEDIA_ID/DHR_A_Class\n",
      "X:  ['dhr', 'a', 'class']\n",
      "Y:  DBPEDIA_ID/Play_Online\n",
      "X:  ['play', 'online']\n",
      "Y:  DBPEDIA_ID/Compañía_Trasatlántica\n",
      "X:  ['compa', '', '', '', 'a', 'trasatl', '', 'ntica']\n",
      "Y:  DBPEDIA_ID/Tyne_Tees_Steam_Shipping_Company\n",
      "X:  ['tyne', 'shipping']\n",
      "Y:  DBPEDIA_ID/John_Chetwynd-Talbot,_1st_Earl_Talbot\n",
      "X:  ['lady', 'charlotte', 'talbot']\n",
      "Y:  DBPEDIA_ID/SARM_Division_No._5\n",
      "X:  ['sarm', 'division', 'no', '', '5']\n",
      "Y:  DBPEDIA_ID/Milano_Vipers\n",
      "X:  ['hc', 'milano', 'vipers']\n",
      "Y:  DBPEDIA_ID/Conchobair_mac_Tiorrdelbach_Ua_Conchobair\n",
      "X:  ['conchobair', 'mac', 'tiorrdelbach', 'ua', 'conchobair']\n",
      "Y:  DBPEDIA_ID/Druga_HNL\n",
      "X:  ['croatian', 'second', 'division']\n",
      "Y:  DBPEDIA_ID/Kearton_Coates\n",
      "X:  ['kearton', 'coates']\n",
      "Y:  DBPEDIA_ID/Nehemiah_Merritt\n",
      "X:  ['nehemiah', 'merritt']\n",
      "Y:  DBPEDIA_ID/Maintenance_of_Certification_for_Surgery#American_Society_of_Colorectal_Surgeons\n",
      "X:  ['maintenance', 'of', 'certification', 'for', 'surgery', 'american', 'society', 'of', 'colorectal', 'surgeons']\n",
      "Y:  DBPEDIA_ID/Ayere_language\n",
      "X:  ['ayere', 'language']\n",
      "Y:  DBPEDIA_ID/Belgian_euro_coins\n",
      "X:  ['belgian', 'coins']\n",
      "Y:  DBPEDIA_ID/Shaft_In_Africa\n",
      "X:  ['shaft', 'in', 'africa']\n",
      "Y:  DBPEDIA_ID/1947-48_Australia_rugby_union_tour_of_the_British_Isles,_Ireland,_France_and_North_America\n",
      "X:  ['1947-48', 'wallabies']\n",
      "Y:  DBPEDIA_ID/Marabina_Jaimes\n",
      "X:  ['marabina', 'jaimes']\n",
      "Y:  DBPEDIA_ID/Children's_television\n",
      "X:  ['children', 's', 'programmes']\n",
      "Y:  DBPEDIA_ID/Spaniacris_deserticola\n",
      "X:  ['spaniacris', 'deserticola']\n",
      "Y:  DBPEDIA_ID/Louver_Drape_window_coverings\n",
      "X:  ['louver', 'drape', 'window', 'coverings']\n",
      "Y:  DBPEDIA_ID/John_Bernard_McDowell\n",
      "X:  ['most', 'reverend', 'john', 'b', '', 'mcdowell']\n",
      "Y:  DBPEDIA_ID/Ward_Prentice\n",
      "X:  ['ward', 'prentice']\n",
      "Y:  DBPEDIA_ID/Eucithara_solida\n",
      "X:  ['eucithara', 'solida']\n",
      "Y:  DBPEDIA_ID/Dyllan_Bernardo_Zimberknopf\n",
      "X:  ['dyllan', 'bernardo', 'zimberknopf']\n",
      "Y:  DBPEDIA_ID/2004_William_&_Mary_Tribe_football_team\n",
      "X:  ['his', 'senior', 'season']\n",
      "Y:  DBPEDIA_ID/University_of_Swaziland\n",
      "X:  ['university', 'of', 'swaziland']\n",
      "Y:  DBPEDIA_ID/Willamette_Valley_Company\n",
      "X:  ['willamette', 'valley', 'company']\n",
      "Y:  DBPEDIA_ID/range_(biology)\n",
      "X:  ['geographical', 'range']\n",
      "Y:  DBPEDIA_ID/St_Neots_Priory\n",
      "X:  ['st', 'neots', 'priory']\n",
      "Y:  DBPEDIA_ID/Global_Gender_Gap\n",
      "X:  ['global', 'gender', 'gap']\n",
      "Y:  DBPEDIA_ID/Eric_Walter_Elst\n",
      "X:  ['e', '', 'w', '', 'elst']\n",
      "Y:  DBPEDIA_ID/Departments_of_the_Continental_Army\n",
      "X:  ['highland', 'department']\n",
      "Y:  DBPEDIA_ID/Dicranomyia_esau\n",
      "X:  ['d', '', 'esau']\n",
      "Y:  DBPEDIA_ID/mass-storage_device\n",
      "X:  ['mass-storage', 'device']\n",
      "Y:  DBPEDIA_ID/92d_Air_Expeditionary_Wing\n",
      "X:  ['92d', 'air', 'expeditionary', 'wing']\n",
      "Y:  DBPEDIA_ID/Charleston_Forge\n",
      "X:  ['charleston', 'forge']\n",
      "Y:  DBPEDIA_ID/Joseph_d'Oultremont\n",
      "X:  ['joseph', 'd', 'oultremont']\n",
      "Y:  DBPEDIA_ID/2016_World_Junior_Figure_Skating_Championships\n",
      "X:  ['2016', 'junior', 'worlds']\n",
      "Y:  DBPEDIA_ID/GNU_Lesser_General_Public_License\n",
      "X:  ['gnu', 'lgpl', 'v2', '1', 'license']\n",
      "Y:  DBPEDIA_ID/Hilton_Hotel\n",
      "X:  ['huntington', 'hilton']\n",
      "Y:  DBPEDIA_ID/Idaho_House_of_Representatives\n",
      "X:  ['idaho', 'state', 'house']\n",
      "Y:  DBPEDIA_ID/Dominican_Broadcasting_Station\n",
      "X:  ['dominican', 'broadcasting', 'station']\n",
      "Y:  DBPEDIA_ID/The_Columbia_Museum_of_Art\n",
      "X:  ['the', 'columbia', 'museum', 'of', 'art']\n",
      "Y:  DBPEDIA_ID/Wicklow_Records\n",
      "X:  ['wicklow', 'records']\n",
      "Y:  DBPEDIA_ID/Hulk_2\n",
      "X:  ['hulk', '2']\n",
      "Y:  DBPEDIA_ID/Linton_(disambiguation)\n",
      "X:  ['linton', '', 'disambiguation']\n",
      "Y:  DBPEDIA_ID/hillside_castle\n",
      "X:  ['hillside', 'castle']\n",
      "Y:  DBPEDIA_ID/Advaita_Vedanta\n",
      "X:  ['advaita', 'vedantist']\n",
      "Y:  DBPEDIA_ID/Anabathron_contabulatum\n",
      "X:  ['anabathron', 'contabulatum']\n",
      "Y:  DBPEDIA_ID/Michael_Reghi\n",
      "X:  ['michael', 'reghi']\n",
      "Y:  DBPEDIA_ID/11_March_2004_Madrid_train_bombings\n",
      "X:  ['train', 'bombings']\n",
      "Y:  DBPEDIA_ID/Walt_Whitman_Park_(Washington_D.C.)\n",
      "X:  ['walt', 'whitman', 'park']\n"
     ]
    }
   ],
   "source": [
    "# Prepare Train_data\n",
    "fname = os.path.join(train_file_path,train_file_name)\n",
    "x_train , y_train = load_data(fname,wordvec) # Preprocess the input data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 21)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = Word2VecTOEmbeddingMatrix(wordvec,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/an/Env/python3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-97ca940d5f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_rnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_of_epochs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-4cc3c7ad4d67>\u001b[0m in \u001b[0;36minit_rnn_model\u001b[0;34m(vocab_size, embedding_dim, embedding_matrix, MAX_SEQUENCE_LENGTH)\u001b[0m\n\u001b[1;32m      8\u001b[0m                             trainable=False)\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add the Embedding layers to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model = init_rnn_model(vocab_size, embedding_dim, embedding_matrix, MAX_SEQUENCE_LENGTH) # Get model architecture\n",
    "history = model.fit(x_train , y_train, epochs = num_of_epochs , batch_size = batch_size, validation_split = 0.1)\n",
    "print('Training Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_weights(save_model_path)\n",
    "print('Saved model to: ', save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
